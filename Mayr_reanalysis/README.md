# Important information regarding the reanalysis of data from Mayr et al. (2018) #

## Overview ##

This folder contains the code to prepocess and analyze the data for our analysis of the original article by Mayr and coworkers,
[Large-scale comparison of machine learning methods for drug target prediction on ChEMBL](https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc00148k#!divAbstract).

**BibTeX:**

>@Article{bib:Mayr2018,\
>&nbsp;&nbsp;author="Mayr, Andreas and Klambauer, G{\\"u}nter and Unterthiner, Thomas and Steijaert, Marvin and Wegner, J{\\"o}rg K. and Ceulemans, Hugo and Clevert, Djork-Arn{\\'e} and Hochreiter, Sepp",\
>&nbsp;&nbsp;title={{Large-scale comparison of machine learning methods for drug target prediction on ChEMBL}},\
>&nbsp;&nbsp;journal="Chem. Sci.",\
>&nbsp;&nbsp;year="2018",\
>&nbsp;&nbsp;volume="9",\
>&nbsp;&nbsp;issue="24",\
>&nbsp;&nbsp;pages="5441-5451"\
>} 

Additional information on their code and dataset creation may be found at http://ml.jku.at/research/lsc/index.html or on their GitHub page https://github.com/ml-jku/lsc . 

Please note that all of the work by Mayr and coworkers is licensed under the GNU General Public License v3.0 as detailed at http://www.bioinf.jku.at/research/lsc/LICENSE and https://github.com/ml-jku/lsc/blob/master/LICENSE .
Much of our work is heavily derved/reliant on their data and code. We have done our best to clearly demarcate which work is their original data/code, and which is our extensions thereof. 

**_All mistakes in the programs included herein should, of course, be assumed to be our own fault._**

***

## Obtaining and preprocessing the data ##

Most of their their data can be downloaded at http://ml.jku.at/research/lsc/mydata.html 
However, for our analysis, we mainly used the data in the `dataPythonReduced` section. 
This data is optimized for those using Python and includes only the data that is directly 
necessary for running their simple feed-forward neural network algorithm. 
This also reduces the memory requirements.

We recommend directly downloading the zip file in the browser: https://ml.jku.at/research/lsc/chembl20/dataPythonReduced.zip
The file https://ml.jku.at/research/lsc/chembl20/dataPythonReduced/chembl20Deepchem.pckl will also need to be downoaded.

Once unzipped, this data is used to construct fingerprints for further analysis/testing of algorithms. Creating this data involves the running of two separate programs: `data_preprocessing/initial_processing.py` and `data_preprocessing/make_sparse_fps.py`. Please note that the first of these programs is essentially copied from a portion of a script from Mayr et al., as is specified in the comments.

The main output of interest from these programs is included in the `data_for_replication` folder one level outside the `data_preprocessing` directory. The whole process is summarized as follows:
1. Download and unzip https://ml.jku.at/research/lsc/chembl20/dataPythonReduced.zip , thus creating a `data_preprocessing/dataPythonReduced` folder inside the `data_preprocessing` directory
2. Download https://ml.jku.at/research/lsc/chembl20/dataPythonReduced/chembl20Deepchem.pckl and place the file inside the recently created `data_preprocessing/dataPythonReduced/` directory.
3. Run the Python script `data_preprocessing/intial_processing.py`
4. Run the Python script `data_preprocessing/make_sparse_fps.py` 

Keep in mind that the size of the data needed and generated by these steps is quite large, and often outside the GitHub storage limits. Therefore we provide the data here (INSERT LINK)

**Statement of modifications**:
- The data in `data_preprocessing/dataPythonReduced` is directly copied from Mayr et al: http://ml.jku.at/research/lsc/mydata.html
- The data in `data_for_replication` is generated by us. 

***

## Testing a model and generating data regarding assay sizes/class imbalance ##

The `test_ml_model` directory contains the script `test_ml_model/run_classifier.py`, which gives an example of how to run a generic Scikit-Learn classifier on the data from Mayr et al. The code replicates their nested cluster-CV procedure using the folds specified by the original authors. 

The output of the program is a pickled Python dictionary, which not only contains the results from the CV procedure, but also information regarding train/test sizes and class imbalance. This information is then included in `/ecfp6/fold_info.csv` and used for further analysis. Importantly the assay specific output of `test_ml_model/run_classifier.py` is not originally in the same order as the data in `/ecfp6/`. The data can be merged using `/ecfp6/labelsWeakHard.targetNames`, which specifies the order of assay from `test_ml_model/run_classifier.py`.

**Statement of modifications**:
- The code in `test_ml_model/run_classifier.py` attempts to replicate the general cross-validation procedure of Mayr et al., but is mostly our own.
- The data generated by `test_ml_model/run_classifier.py` and tabulated in `/ecfp6/fold_info.csv` is not provided by the original authors. However, we note that similar information can be found in [Supplementary Table 10](http://www.rsc.org/suppdata/c8/sc/c8sc00148k/c8sc00148k2.csv) described by the authors in the [Electronic Supplementary Information](http://www.rsc.org/suppdata/c8/sc/c8sc00148k/c8sc00148k1.pdf) 

***

## Analyzing the data ## 

In this section we rely heavily on the data provided by Mayr and coworkers at http://www.bioinf.jku.at/research/lsc/aucPerformancesPerMethod.zip , which describes the AUC-ROC performance of each method on each assay. Additionally, we make use of tables 12 and 13, as described in the [Supplementary information](http://www.rsc.org/suppdata/c8/sc/c8sc00148k/c8sc00148k1.pdf) to compare the AUC-ROC and AUC-PRC performances of the FNN method. These tables are provided by the authors at http://www.rsc.org/suppdata/c8/sc/c8sc00148k/c8sc00148k4.csv and http://www.rsc.org/suppdata/c8/sc/c8sc00148k/c8sc00148k5.csv . We also make heavy use of the aformentioned `/ecfp6/fold_info.csv` file. 

If you wish to explore the data yourself, we highly suggest the use of [Google Colab](https://colab.research.google.com/). Included in this repo is the `Mayr_reanalysis/colab_example.ipynb` notebook, with a direct link to opening the notebook in Colab. The notebook gives an example of how to load the data and uses the excellent [Altair](https://altair-viz.github.io/index.html) visualization library for data exploration. Using Colab will also avoid the annoyances of installing dependencies.

Any of the notebooks in this repo may also be opened in Colab directly from GitHub by using the `File/Open notebook...` tab in Colab. 

**Statement of modifications**:
- The code in the provided notebooks is almost entirely our own.
- All of the data used in this analysis is directly from the original authors, except for that in `/ecfp6/fold_info.csv`, which was generated by us.






